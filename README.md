Skip Grams
==========
ref : - http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf
"a technique where by n-grams are still stored to model language, but they allow for tokens to be skipped"

Skip Grams is a fairly simple concept and a very efficient one. The whole idea is to generate more features from a sentence. For example lets consider a bigram split of the sentence : -

 "charlie hebdo cartoonist shot by islamic extremists."
 
 The following list of Bigrams are generated by nltk : -
        
        [('charlie', 'hebdo'),
         ('hebdo', 'cartoonist'),
         ('cartoonist', 'shot'),
         ('shot', 'by'),
         ('by', 'islamic'),
         ('islamic', 'extremists'),
         ('extremists', '.')]

Now what k-skip gram algorithm is going to do is skip kth word and generate another bigram.
In addition to the one's above, you might see something like : -

        [('charlie', 'cartoonist'),
         ('hebdo', 'shot'),
         ...]
So instead of having your usual list of bigrams you will be getting more bigram combinations that will add more features to your model.The reason why one would like to do that is there can never be enough data for a language model.

 Implementation using list comprehension
---------------------------------------

    sent = 'Hi, I will be there in sometime.'
    # assuming a space tokenizer
    tokens = sent.split()
    # ----> 1-skip-bigram
    print [(tokens[index], tokens[index+j]) for index in range(len(tokens)) for j in range(1,3) if (index + j) < len(tokens)]
    # ---> 2-skip-bigram by changing the range j
    print [(tokens[index], tokens[index+j]) for index in range(len(tokens)) for j in range(1,4) if (index + j) < len(tokens)]
    # ---> 1-skip-trigram by adding another token[...]
    print [(tokens[index], tokens[index + j], tokens[index + j + 1]) for index in range(len(tokens)) for j in range(1,3) if (index + j + 1) < len(tokens)]
    

This repo is an attempt to vectorize this. There might be libraries available to do the same.